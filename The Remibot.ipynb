{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f6776d2-40ce-433f-92c0-ab6ad5916f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from nltk.stem import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09776daa-e8e3-4167-b727-8f00395212c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text):\n",
    "    text=str(text).lower()\n",
    "    special_char_text=re.sub(r'[^a-z]',' ',text)\n",
    "    tokenized_text=nltk.word_tokenize(special_char_text)\n",
    "    lemmatized_text=wordnet.WordNetLemmatizer()\n",
    "    tags_list=pos_tag(tokenized_text,tagset=None)\n",
    "    lema_words=[]\n",
    "    for token,pos_token in tags_list:\n",
    "        if pos_token.startswith('V'):\n",
    "            pos_val='v'\n",
    "        elif pos_token.startswith('J'):\n",
    "            pos_val='a'\n",
    "        elif pos_token.startswith('R'):\n",
    "            pos_val='r'\n",
    "        else:\n",
    "            pos_val='n'\n",
    "        lema_token=lemmatized_text.lemmatize(token,pos_val)\n",
    "        lema_words.append(lema_token)\n",
    "        \n",
    "    return \" \".join(lema_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aabe83b2-041c-4e4b-ad36-6502c1f5bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_of_Input(u_input):\n",
    "    q=[]\n",
    "    a=u_input.split()\n",
    "    for i in a:\n",
    "        if i in stop:\n",
    "            continue\n",
    "        else:\n",
    "            q.append(i)\n",
    "        b=\" \".join(q)\n",
    "    global ques_lema\n",
    "    global ques_bow\n",
    "    ques_lema=text_normalization(b)\n",
    "    ques_bow=cv.transform([ques_lema]).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b992c5e-0261-470a-a52f-280e0d4f48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_tfidf(text):\n",
    "    lemma=text_normalization(text)\n",
    "    tf=tfidf.transform([lemma]).toarray()\n",
    "    cos=1-pairwise_distances(df_tfidf,tf,metric='cosine')\n",
    "    index_value=cos.argmax()\n",
    "    return [df['Remedies'].loc[index_value]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbc420e5-e2f6-47d3-aea7-9669a72aad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Greeting_Inputs = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\", \"Could you help me\")\n",
    "Greeting_Responses = [\"Hi. I will assist you.\", \"Hey. I will assist you.\", \"*nods*\", \"Hi there. I will assist you.\", \"Hello. I will assist you.\", \"I am glad you are talking to me. I will assist you.\"]\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in Greeting_Inputs:\n",
    "            return random.choice(Greeting_Responses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "985f1209-2dc1-43c1-b14a-2e0da1aa0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"remibot_dataset_1614271001.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bad4a81-4bb8-4f42-9c07-0c26e1b4c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_symptoms']=df['Symptom'].apply(text_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f84d380-3e67-4dcc-b4f9-df82d72aa486",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')\n",
    "stop_words = ['?',',','.','!','(',')']\n",
    "for i in stop_words:\n",
    "    stop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6602ec8-5a28-4756-ac8f-6d9df4706c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remibot: Hello I am Remibot! You can Type the symptoms that you are feeling.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: cough\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remibot: The Natural Remedies for the following Symptoms are:\n",
      " Caffeinated tea or coffee,  Lavender oil\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: cold, cough, fever\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remibot: The Natural Remedies for the following Symptoms are:\n",
      " ginger, garlic, turmeric\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye. Take care...\n"
     ]
    }
   ],
   "source": [
    "continue_diag = True\n",
    "print(\"Remibot: Hello I am Remibot! You can Type the symptoms that you are feeling.\")\n",
    "\n",
    "while(continue_diag==True):\n",
    "    user_input = input(\"User:\")\n",
    "    ques = user_input.lower()\n",
    "      \n",
    "    cv=CountVectorizer()\n",
    "    x=cv.fit_transform(df['lemmatized_symptoms']).toarray()\n",
    "    \n",
    "    splitting_of_Input(ques)\n",
    "    \n",
    "    tfidf=TfidfVectorizer()\n",
    "    x_tfidf=tfidf.fit_transform(df['lemmatized_symptoms']).toarray()\n",
    "    \n",
    "    df_tfidf=pd.DataFrame(x_tfidf,columns=tfidf.get_feature_names_out())\n",
    "    ques_tfidf=tfidf.transform([ques_lema]).toarray()\n",
    "  \n",
    "    \n",
    "    features=cv.get_feature_names_out()\n",
    "    df_bow=pd.DataFrame(x,columns=features)\n",
    "\n",
    "    cosine_value=1-pairwise_distances(df_bow,ques_bow,metric='cosine')\n",
    "    index_value=cosine_value.argmax()\n",
    "    if ques != \"bye\":\n",
    "        if ques == \"thanks\" or ques == \"thank you\" or ques == \"thank you so much\" or ques == \"thank you very much\":\n",
    "            continue_diag = False\n",
    "            print(\"Remibot: Most Welcome\")\n",
    "        else:\n",
    "            if greeting(ques) != None:\n",
    "                print(f\"Remibot: {greeting(ques)}\")\n",
    "            else:\n",
    "                for k in chat_tfidf(ques):\n",
    "                    var_n = (f\"The Natural Remedies for the following Symptoms are:\\n {k}\")   \n",
    "                print(f\"Remibot: {var_n}\")\n",
    "    else:\n",
    "        continue_diag = False\n",
    "        print(\"Bye. Take care...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49599e1e-da06-46bb-a259-6cbecc000bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
